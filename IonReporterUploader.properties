# Upload file type.
# Valid options are both, bam_only, and vcf_only.
upload.file.type=bam_only

# Number of parallel network streams. proton block runs are already parallel in nature.
# So please do not increase the additional parallelism within the uploader for proton 
# runs.  A value of 5 is optimum for pgm runs, and a value of 1 is optimum for proton runs 
# A value of 5 is optimum for iru-cli and iru-app runs. 
# IRU app uses same keys as that of IRU-cli. The values for cli can be overridden
# for different kinds of OS. 32bit Windows does show some issues for acquiring large 
# contiguous memory by jvm. Hence cli running on windows should use lower number. A value of 
# 3 is optimum for cli running on windows.
cli.numParallelStreams=5
cli.win.numParallelStreams=3
cli.linux.numParallelStreams=5
cli.mac.numParallelStreams=5
pgm.numParallelStreams=5
proton.numParallelStreams=5


# segment size, for each file segment in a parallel stream. optimum value of 128MB 
# for pgm runs, 16MB for proton runs and 64MB for iru-cli or iru-app runs. 
# IRU app uses same keys as that of cli. The values for cli can be overridden
# for different kinds of OS. 32bit Windows does show some issues for acquiring large 
# contiguous memory by jvm. Hence cli running on windows should use lower number. A value of 
# 32MB  is optimum for cli running on windows.
cli.fileSegmentSize=128MB
cli.win.fileSegmentSize=32MB
cli.linux.fileSegmentSize=128MB
cli.mac.fileSegmentSize=128MB
pgm.fileSegmentSize=128MB
proton.fileSegmentSize=128MB


#overrides any of the above number of parallel streams and file segment sizes, if the
# IR server is a local IR server(not cloud based). Owing to more number of TS
#connecting to same IR, and IR doesnt have enough memory compared to cloud.
local.ir.numParallelStreams=5
local.ir.fileSegmentSize=128MB



# retry counts. A file is broken down into segments, if file size is greater than a 
# certain threshold size.  Each segment upload will be retried 12 times by default.
# Each file, as such will be tried 8 times by default, besides individual segment
# retries of each file. 
# Data Transfer of files is associated with a number of control signal ws-api calls
# that go before, during and after the data transfer. Those control signals will 
# also be retried 8 times by default.
# Use the following parameters to over ride the retry counts. 
file.upload.retry.count=8
segment.upload.retry.count=12
txfr.ctrl.api.retry.count=8

# time out value (in seconds) for webservice calls made from IRU to the IR
#Not implemented yet.
IRWebServiceCallTimeOut=300

# bam files containing multiple read groups, are detected and not allowed to be
# uploaded by the iru-cli and iru-app. IR analysis fails on such .bam files.
# Valid values are true or false (no quotes). Default is false.
# If user wants to upload such files for future use, then user can turn on 
# this option. Highly recommended that this be kept as false.
cli.multiple.RG.bam.uploads.allowed=false

# Time out value (in minutes) for upload assembly in case of IR-local or
# IR-cloud-two-stage modes. Direct S3 uploads are not affected by this 
# timeout. Changing this is usually not recommended. Usually, even large 
# files will be completing the assembly in maximum of 10 minutes, if the 
# IR workload is moderate. If it takes too much of time to assemble files,
# then it may mean that IR is already undergoing heavy workload and its 
# better to wait for sometime and then retry the uploads, instead of 
# uploading more and more data by increasing this timeout.  The default 
# is 4:00 Hrs, which is 240 minutes
# upload.assembly.timeout=240
upload.assembly.timeout=240


# Use this option only if the direct Internet connection has trouble 
# and s3 uploads are failing frequently. Two stage uploads undergoes 
# retries as specified in   file.upload.retry.count   key.  For  IRU
# versions 5.2 and above,  direct s3  will be attempted  only a  few
# times as configured in the retries below,and then it automatically 
# switches to the two stage s3 upload method, even if this is set to
# false.
slower.two.stage.S3.upload=false

# Direct S3 file level retries before automatically switching to two stage s3 upload.
# Automatic fall back to two stage mode is active,  if slower.two.stage.s3.upload key 
# is set to false.  If it was already  set to true,  then direct s3 is not attempted,
# and it starts trying only the two stage upload method. After switching to two stage
# upload mode,  it undergoes another a series of retries  meant for two stage upload, 
# which is as specified in file.upload.retry.count key.  
# Default direct S3 retry is 3.
retry.direct.s3.before.switching.to.two.stage.s3.upload = 3


# For Annotation workflows, do not upload the .bam files by default.
upload.bam.files.for.annotation.workflows=false

###################################################
###################################################
##  
##     AmazonAWS S3 storage access parameters  (for direct S3 uploads)
##  
# max http connection
# maximum number of S3 open http request pool size. default is 50 from amazon.
amazonaws.s3.max.http.connection = 50
# connection timeout 
# timeout for creating new S3 connections, in milliseconds. default is 
# 50000 (50 seconds) from amazon.
# 1800000 = 1800 seconds = 30 minutes
amazonaws.s3.connection.timeout = 1800000
# socket timeout 
# timeout for reading from an S3 conected socket, in milliseconds. default is 
# 50000 (50 seconds) from amazon.
# 1800000 = 1800 seconds = 30 minutes
amazonaws.s3.socket.timeout = 1800000
# connection ttl
# timeout for reading from an S3 conected socket, in milliseconds. default is 
# -1 from amazon.
#amazonaws.s3.connection.ttl = -1
# tcp keepalive
# whether to keep alive tcp connection on S3 conected socket, true / false, default is 
# false from amazon.
amazonaws.s3.tcp.keepalive = true
# max error retry
# The maximum number of times, to retry a failed upload. default is ___ from amazon.
amazonaws.s3.max.error.retry = 15


# allow user to upload report.pdf based on the below property
upload.report.pdf = true

# disable multi workflow selection validation for application type
application.type=ImmuneRepertoire